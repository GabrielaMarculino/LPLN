# -*- coding: utf-8 -*-
"""Exemplos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17dqVJsQv3esM0osQoufeE7DCJ9oNEeKg

#Exemplo 01 - Expressão Regular
##Introdução à Processamento de Linguagem Natural

#Observações
Esse conjunto de códigos são de autoridade do Professor Dr. Evandro Cunha e Professor Dr. Thiago Castro Ferreira, pesquisadores da área de Processamento de Linguagem Natural da Universidade Federal de Minas Gerais (UFMG).

Eles se encontram disponíveis nesse conjunto de videoaulas disponibilizadas no youtube: https://www.youtube.com/watch?v=NBqDzewxJDo&list=PLt2qoMeOJsQyAklWpREY5ivAUe3jrrR21

Esses códigos foram utilizados para exemplificar na prática a utilização do Processamento de Linguagem Natural no meio computacional durante um minicurso para os cursos de computação da Universidade Estadual do Mato Grosso do Sul (UEMS) e Universidade Federal da Grande Dourados (UFGD).
"""

import re # biblioteca utilizada para definir expressões regulares

poema = """Ainda que mal

Ainda que mal pergunte,
ainda que mal respondas;
ainda que mal te entenda,
ainda que mal repitas;
ainda que mal insista,
ainda que mal desculpes;
ainda que mal me exprima,
ainda que mal me julgues;
ainda que mal me mostre,
ainda que mal me vejas;
ainda que mal te encare,
ainda que mal te furtes;
ainda que mal te siga,
ainda que mal te voltes;
ainda que mal te ame,
ainda que mal o saibas;
ainda que mal te agarre,
ainda que mal te mates;
ainda assim te pergunto
e me queimando em teu seio,
me salvo e me dano;"""

re.findall(r"mal", poema) # método findall retorna todas as ocorrências de um padrão numa determinada string

# len(re.findall(r"mal", poema))

"""#Exemplo 2 - Disjunção"""

import re # biblioteca utilizada para definir expressões regulares

poema = """Ainda que mal

Ainda que mal pergunte,
ainda que mal respondas;
ainda que mal te entenda,
ainda que mal repitas;
ainda que mal insista,
ainda que mal desculpes;
ainda que mal me exprima,
ainda que mal me julgues;
ainda que mal me mostre,
ainda que mal me vejas;
ainda que mal te encare,
ainda que mal te furtes;
ainda que mal te siga,
ainda que mal te voltes;
ainda que mal te ame,
ainda que mal o saibas;
ainda que mal te agarre,
ainda que mal te mates;
ainda assim te pergunto
e me queimando em teu seio,
me salvo e me dano;"""

re.findall(r"[Aa]inda que mal", poema) # método findall retorna todas as ocorrências de um padrão numa determinada string
             # [Aa] - disjunção
# len(re.findall(r"[Aa]inda que mal", poema))



"""#Exemplo 3 - Disjunção e intervalos"""

import re

descricao = """Monitor LG 19.5'' LED HD - HDMI, 2ms, Ajuste de Inclinação, Reader Mode, 4-Screen Split, - 20MK400H-B"""

re.findall(r"[0-4]", descricao)

# len(re.findall(r"[0-4]", descricao))

"""#Exemplo 4 - Negação na Disjunção"""

import re

poema = """Quando Ismália enlouqueceu,
Pôs-se na torre a sonhar…
Viu uma lua no céu,
Viu outra lua no mar.

No sonho em que se perdeu,
Banhou-se toda em luar…
Queria subir ao céu,
Queria descer ao mar…

E, no desvario seu,
Na torre pôs-se a cantar…
Estava perto do céu,
Estava longe do mar…

E como um anjo pendeu
As asas para voar…
Queria a lua do céu,
Queria a lua do mar…

As asas que Deus lhe deu
Ruflaram de par em par…
Sua alma subiu ao céu,
Seu corpo desceu ao mar…"""

re.findall(r"[^a-z]", poema) # todos os elementos exceto caracteres em letra maiúscula

"""#Exemplo 5 - Múltiplos Padrões de Busca"""

import re

poema = """No meio do caminho tinha uma pedra
tinha uma pedra no meio do caminho
tinha uma pedra
no meio do caminho tinha uma pedra.

Nunca me esquecerei desse acontecimento
na vida de minhas retinas tão fatigadas.
Nunca me esquecerei que no meio do caminho
tinha uma pedra
tinha uma pedra no meio do caminho
no meio do caminho tinha uma pedra."""

re.findall(r"pedra|[Tt]inha", poema)  # busca todas as combinações de tinha e pedra

"""#Exemplo 6 - Quantificadores"""

import re

caso = """Pode checar a conta ABC4323?
          Emitir segunda via do boleto de Dezembro da conta YIC9834.
          Meu nome é Gabriela, portador da conta NHD8432. Quantos créditos tenho?"""

re.findall(r"\b[A-Z]{3}[0-9]{4}\b", caso) # filtra tudo que tem sequência de 3 letras maiúsculas e 4 números de 0 à 9

"""#Exercicio: Como retornar os padrões de risos em mensagens de redes sociais? Como: Hahahaha | Huahuahua | Haha | KKK"""

import re

expressoes = """Hahahahaha
                Huahuahuahua
                Haha
                kkkkk"""

p = re.compile(r"k+|([Hh|a])+|([Hh|ua])+") # Podemos combinar uma expressão regular com objetos de padrões, que pode ser usado para encontrar padrões.
iterador = p.finditer(expressoes) # encontra todas as substrings que têm correspondência com a expressão regular, e as retorna como um iterador

for i in iterador:
  inicio, fim = i.span() #  forma bem útil e eficiente para coletar informações em strings, encontrar padrões de textos
  print(inicio, fim, expressoes[inicio:fim])

"""#Exemplo 7 - Substituição"""

import re

texto = """Reconheço que tomei decisões insatisfatórias recentemente, mas posso lhe garantir com absoluta certreza que meu trabalho voltará ao normal."""

re.sub(r'[,;.!\n]','',texto) # substitui todos os parâmetros por um caractere vazio

"""#Exemplo 8 - Memória"""

import re

texto = """Reconheço que tomei decisões insatisfatórias recentemente, mas posso lhe garantir com absoluta certreza que meu trabalho voltará ao normal."""

print(re.sub(r'([,;.!?:])',r' \1', texto)) # adiciona um espaço entre as pontuações

"""#Exemplo - Distância de Edição"""

from nltk.metrics import *  # estabelece uma infraestrutura que pode ser utilizada para criar programas de NLP


a = "Bello Horizonte"
b = "Belo Horizonte"

distancia = edit_distance(a,b)

print("A distância de Levenstein entre ", a, "e", b, "é igual a", distancia)

"""#Exemplo - Tokenização"""

import nltk
nltk.download('punkt')
from nltk import tokenize

versos = """No meio do caminho tinha uma pedra
            Tinha uma pedra no meio do caminho""" # lembrar de falar da letra maiuscula/minuscula

palavras = tokenize.word_tokenize(versos, language='portuguese')

print(len(palavras), palavras, '\n', len(set(palavras)), set(palavras))

"""#Exemplo de Capitalização"""

import nltk
nltk.download('punkt')
from nltk import tokenize

versos = """No meio do caminho tinha uma pedra
            Tinha uma pedra no meio do caminho"""

palavras = tokenize.word_tokenize(versos.lower(), language='portuguese')
                                  # transformando todos os caracteres em minusculo

print(len(palavras), palavras, '\n', len(set(palavras)), set(palavras))

# essa etapa é importante e muito aplicada, entretanto temos que ir com cautela pois dependendo da situação essa etapa atrapalha, pois
# imagine a seguinte problemática: utilizamos o prefixo (se) joao fosse à escola e no mesmo contexto
# estivéssimos usando a sigla (SE) para nos referir a Sergipe, esse tratamento de dados seria totalmente erroneo!

"""#Exemplo de Lematização"""

# download das bibliotecas
!pip install -U pip setuptools wheel
!pip install -U spacy
!pip install -U spacy-lookups-data
!python -m spacy download pt_core_news_sm

import spacy # transforma dados / tokenização

pln = spacy.load('pt_core_news_sm')

frase = pln("A comida estava gostosa. Todos comeram e gostaram.")

lemas = []

for token in frase:
  lema = token.lemma_
  lemas.append(lema)

print(lemas)

"""#Exemplo de Radicalização - Stemming"""

# Download da biblioteca
nltk.download('rslp')

import nltk

raiz = nltk.stem.RSLPStemmer()

print(raiz.stem("a"),
raiz.stem("comida"),
raiz.stem("estava"),
raiz.stem("gostosa"),
raiz.stem("todos"),
raiz.stem("comeram"),
raiz.stem("e"),
raiz.stem("gostaram"))

"""#Exemplo - Segmentação de Frases"""

# download da biblioteca
nltk.download('punkt')

import nltk

frases = nltk.data.load("tokenizers/punkt/portuguese.pickle")

citacao_darcy = """ Fracassei em tudo o que tentei na vida. Tentei alfabetizar as crianças brasileiras, não consegui.
Tentei salvar os índios, não consegui. Tentei fazer uma universidade séria e fracassei.
Tentei fazer o Brasil desenvolver-se automaticamente e fracassei. Mas os fracassos são minhas vitórias.
Eu detestaria estar no lugar de quem me venceu"""

frases.tokenize(citacao_darcy)

"""#Implementação completa

#Baixando a versão 3.5 do NLTK
"""

!pip3 install nltk == 3.5

"""#Importando dependências"""

import nltk
nltk.download('punkt')

"""#Passo 1: Carregando Córpus"""

texto = """No meio do caminho tinha uma pedra
Tinha uma pedra no meio do caminho
Tinha uma pedra
No meio do caminho tinha uma pedra"""

texto = texto.lower().split('\n')

"""#Passo 2: Tokenizando as Sentenças dos corpos"""

texto_tok = []

for verso in texto:
  tokens = nltk.word_tokenize(verso, language = 'portuguese')
  texto_tok.append(tokens)

  texto_tok

"""#Passo 3: Pré-processando sentenças"""

from nltk.lm.preprocessing import pad_both_ends

ngramas = 2

texto_tok_pad = []
for verso in texto_tok:
  padded = list(pad_both_ends(verso, n = ngramas))
  texto_tok_pad.append(padded)

texto_tok_pad

"""#Passo 4: Calculando os N-Gramas"""

ngramas = 2

bigramas_pad = []
for verso in texto_tok_pad:
  bigramas = list(nltk.ngrama(verso, n = ngramas))
  bigramas_pad.append(bigramas)

"""#calculando unigramas além dos bigramas"""

from nltk.util import everygrams

ngramas = 2
ngramas_pad = []

for verso in texto_tok_pad:
  ngramas = list(everygrams(verso, max_len=ngramas))
  ngramas_pad.append(ngramas)

ngramas_pad

"""#Passo 5: Colocando todos os tokens do córpus numa única lista"""

from nltk.lm.preprocessing import flatten

tokens = list(flatten(texto_tok_pad))
tokens

"""#Passo 6: Definindo Vocabulário"""

from nltk.lm import Vocabulary

vocab = Vocabulary(tokens, unk_cutoff=1)

"""#procurando uma palavra no vocabulário"""

vocab.lookup("caminho")

"""#obtendo as frequências das palavras do córpus com o comando counts"""

vocab.counts

"""#simplificando o pré-processamento"""

from nltk.lm.preprocessing import padded_everygram_pipeline

ngramas = 2

ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)

"""#Passo 7: Treinando um modelo de lingugem"""

from nltk.lm import MLE

ngramas = 2
lm = MLE(ngramas)
lm.fit(ngramas_pad, vocab)

"""#dado o tokes <s>, gerando um texto de 4 tokens com o modelo de linguegem treinado"""

lm.generate(4, text_seed = ["<s>"])

"""#probabilidade da palavra no:"""

lm.score("no"), lm.logscore("no")

"""#probabilidade da palavra tinha dado a palavra caminho:"""

lm.score("tinha", context = ['caminho']), lm.logscore("tinha", context=["caminho"])
